{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddc3406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Environment\n",
    "# pip install -q scikit-learn pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "import_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# custom loader\n",
    "from dao_original_text_punct import get_raw_chapters\n",
    "raw_chapters = get_raw_chapters()\n",
    "HERE = Path.cwd().parent                         \n",
    "# HERE = Path(__file__).parent.resolve()      # 本地脚本\n",
    "data_dir = HERE / 'data'\n",
    "out_dir = HERE / 'out'\n",
    "data_dir.mkdir(exist_ok=True) \n",
    "out_dir.mkdir(exist_ok=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract_title",
   "metadata": {},
   "source": [
    "# 1. 提取段心 & 生成问句  \n",
    "Extract core sentence & Yixi question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rule_md",
   "metadata": {},
   "source": [
    "规则 Rules：  \n",
    "1. 首句即「故/是以/夫唯」→ 直接采  \n",
    "2. 非首句但以「故/是以/夫唯」开头 → 采  \n",
    "3. 末句收束（排比/反问）→ 采  \n",
    "4. 兜底首句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "core_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_core_sentence(chap: str) -> str:\n",
    "    sents = [s.strip() + '。' for s in re.split(r'[。？]', chap) if s.strip()]\n",
    "    # rule 1\n",
    "    if re.match(r'^故|^是以|^夫唯', sents[0]):\n",
    "        return sents[0]\n",
    "    # rule 2\n",
    "    for s in sents[1:]:\n",
    "        if re.match(r'^故|^是以|^夫唯', s):\n",
    "            return s\n",
    "    # rule 3\n",
    "    last = sents[-1]\n",
    "    if re.search(r'正言若反|是谓|故', last) or '；' in last or '？' in last:\n",
    "        return last\n",
    "    # rule 4\n",
    "    return sents[0]\n",
    "\n",
    "chapters_punct = [c.strip() for c in raw_chapters]\n",
    "core_sentences = [get_core_sentence(c) for c in chapters_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "core_out",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter_id</th>\n",
       "      <th>core_sentence_zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>故常无欲，以观其妙；常有欲，以观其徼。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>故有无相生，难易相成，长短相形，高下相倾，音声相和，前后相随。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>是以圣人之治，虚其心，实其腹；弱其志，强其骨。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chapter_id                 core_sentence_zh\n",
       "0           1              故常无欲，以观其妙；常有欲，以观其徼。\n",
       "1           2  故有无相生，难易相成，长短相形，高下相倾，音声相和，前后相随。\n",
       "2           3          是以圣人之治，虚其心，实其腹；弱其志，强其骨。"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成 core-sentence TSV（ bilingual header ）\n",
    "core_df = pd.DataFrame({'chapter_id': range(1, 82),\n",
    "                        'core_sentence_zh': core_sentences})\n",
    "core_df.to_csv(data_dir/'core_sentences_81.tsv', sep='\\t', index=False)\n",
    "core_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yixi_title",
   "metadata": {},
   "source": [
    "# 2. 手工尹喜问句  \n",
    "Yixi questions (manually created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "yixi_load",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter_id</th>\n",
       "      <th>core_sentence_zh</th>\n",
       "      <th>core_sentence_en</th>\n",
       "      <th>yixi_question_zh</th>\n",
       "      <th>yixi_question_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>故常无欲，以观其妙；常有欲，以观其徼。</td>\n",
       "      <td>Hence always without desire, one observes its ...</td>\n",
       "      <td>敢问无欲何以观妙，有欲何以观徼？</td>\n",
       "      <td>Dare I ask: how does observing without desire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>故有无相生，难易相成，长短相形，高下相倾，音声相和，前后相随。</td>\n",
       "      <td>Being and non-being create each other; difficu...</td>\n",
       "      <td>敢问有无何以相生？</td>\n",
       "      <td>Dare I ask: how do being and non-being give bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>是以圣人之治，虚其心，实其腹；弱其志，强其骨。</td>\n",
       "      <td>Therefore the sage governs by emptying the min...</td>\n",
       "      <td>敢问虚心弱志何以成治？</td>\n",
       "      <td>Dare I ask: how does emptying the mind and wea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chapter_id                 core_sentence_zh  \\\n",
       "0           1              故常无欲，以观其妙；常有欲，以观其徼。   \n",
       "1           2  故有无相生，难易相成，长短相形，高下相倾，音声相和，前后相随。   \n",
       "2           3          是以圣人之治，虚其心，实其腹；弱其志，强其骨。   \n",
       "\n",
       "                                    core_sentence_en  yixi_question_zh  \\\n",
       "0  Hence always without desire, one observes its ...  敢问无欲何以观妙，有欲何以观徼？   \n",
       "1  Being and non-being create each other; difficu...         敢问有无何以相生？   \n",
       "2  Therefore the sage governs by emptying the min...       敢问虚心弱志何以成治？   \n",
       "\n",
       "                                    yixi_question_en  \n",
       "0  Dare I ask: how does observing without desire ...  \n",
       "1  Dare I ask: how do being and non-being give bi...  \n",
       "2  Dare I ask: how does emptying the mind and wea...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取已手工完成的 yao_question_bank_81.tsv\n",
    "qa_df = pd.read_csv(data_dir/'yao_question_bank_81.tsv', sep='\\t')\n",
    "qa_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reorder_title",
   "metadata": {},
   "source": [
    "# 3. 语义重排  \n",
    "Semantic reordering (TF-IDF + cosine + MST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d03aac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_factory(text_series: pd.Series, label: str, out_dir: Path):\n",
    "    \"\"\"\n",
    "    输入：81 章字符串 Series\n",
    "    返回：dict 含索引、tau、相邻余弦 + 写入 csv\n",
    "    \"\"\"\n",
    "    # 1. 向量化 → 相似度\n",
    "    vec   = CountVectorizer(\n",
    "        tokenizer=lambda s: re.findall(r'[\\u4e00-\\u9fff]', s),\n",
    "        ngram_range=(1,2),\n",
    "        min_df=1,\n",
    "        binary=True)\n",
    "    tfidf = vec.fit_transform(text_series)\n",
    "    sim   = cosine_similarity(tfidf)\n",
    "\n",
    "    # 2. MST → 拓扑序\n",
    "    mst   = minimum_spanning_tree(1 - sim)\n",
    "    order = np.argsort(mst.toarray().sum(axis=1))\n",
    "    order = order[::-1]\n",
    "    # 3. 指标\n",
    "    tau  = pd.Series(np.arange(81)).corr(pd.Series(order), method='kendall')\n",
    "    adj_cos = np.mean([sim[i, j] for i, j in zip(order, order[1:])])\n",
    "\n",
    "    # 4. 写出索引 csv\n",
    "    out_csv = out_dir / f'reorder_index_{label}.csv'\n",
    "    pd.DataFrame({'chapter_id': np.arange(1, 82),\n",
    "                  'new_order': order + 1}).to_csv(out_csv, index=False)\n",
    "\n",
    "    return {'label': label, 'tau': tau, 'adj_cos': adj_cos,\n",
    "            'index': order, 'sim_mat': sim}   # 如需后续画图可继续用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "22244c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C  τ = 0.231   相邻余弦 = 0.0925\n",
      " D  τ = 0.271   相邻余弦 = 0.2328\n",
      " E  τ = 0.180   相邻余弦 = 0.1050\n",
      " F  τ = 0.155   相邻余弦 = 0.1445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Documents/dao_ask_lock/venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    'C': qa_df['core_sentence_zh'],\n",
    "    'D': qa_df['yixi_question_zh'] + ' ' + qa_df['core_sentence_zh'],  # 段心+问句\n",
    "    'E': pd.Series(chapters_punct),                       # 全文\n",
    "    'F': qa_df['yixi_question_zh'] + ' ' + pd.Series(chapters_punct),  # 补问+全文\n",
    "    # 未来加 G/H 只需在这里继续 append\n",
    "}\n",
    "\n",
    "results = []\n",
    "for lbl, txt in inputs.items():\n",
    "    res = reorder_factory(txt, lbl, data_dir)\n",
    "    results.append(res)\n",
    "    print(f\"{lbl:>2}  τ = {res['tau']:.3f}   相邻余弦 = {res['adj_cos']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c82f78e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6 份重排终版已生成 → out/V*_final.txt\n"
     ]
    }
   ],
   "source": [
    "# 6 组「重排索引」字典\n",
    "orders = {\n",
    "    'A': np.arange(81),                                    # 1:1\n",
    "    'B': np.arange(81),                                    # 1:1\n",
    "    'C': pd.read_csv(data_dir / 'reorder_index_C.csv')['new_order'].values - 1, # 段心\n",
    "    'D': pd.read_csv(data_dir / 'reorder_index_D.csv')['new_order'].values - 1,  # 段心+问句\n",
    "    'E': pd.read_csv(data_dir / 'reorder_index_E.csv')['new_order'].values - 1,\n",
    "    'F': pd.read_csv(data_dir / 'reorder_index_F.csv')['new_order'].values - 1,\n",
    "}\n",
    "\n",
    "# 输入文本\n",
    "inputs_raw = {\n",
    "    'A': chapters_punct,\n",
    "    'B': [q + '\\n' + chap  for chap, q in zip(chapters_punct, qa_df['yixi_question_zh'])], #补问+段心 \n",
    "    'C': qa_df['core_sentence_zh'],                                    # 段心\n",
    "    'D': qa_df['yixi_question_zh'] + ' ' + qa_df['core_sentence_zh'], # 段心+问句\n",
    "    'E': pd.Series(chapters_punct),                                    # 全文\n",
    "    'F': qa_df['yixi_question_zh'] + ' ' + pd.Series(chapters_punct),  # 全文+问句\n",
    "}\n",
    "\n",
    "for lbl in ['A','B','C','D','E','F']:\n",
    "    # 1. 按索引重排\n",
    "    reord_txt = [inputs_raw[lbl][i] for i in orders[lbl]]\n",
    "    # 2. 写文件（带章节号，方便后续切分）\n",
    "    blocks = [f'--- {i:02d} ---\\n{chap}' for i, chap in enumerate(reord_txt, 1)]\n",
    "    (out_dir / f'V{lbl}_final.txt').write_text('\\n'.join(blocks), encoding='utf-8')\n",
    "    \n",
    "    # txt_save = open(out_dir / f'V{lbl}_final.txt').read().split('--- ')[1].split('\\n', 1)[1].rstrip('\\n')\n",
    "    # txt_reord = inputs_raw[lbl][orders[lbl][0]]\n",
    "    # print('保存首章 == 重排首章:', txt_save == txt_reord)\n",
    "\n",
    "print('✅ 6 份重排终版已生成 → out/V*_final.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec1ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eval_title",
   "metadata": {},
   "source": [
    "# 4. 评估  \n",
    "Evaluation: Kendall τ & character-level smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a1b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group    Smooth    AdjCos\n",
      "0     A  0.130081  0.112247\n",
      "1     B  0.173294  0.155404\n",
      "2     C  0.090813  0.092548\n",
      "3     D  0.229990  0.232792\n",
      "4     E  0.123440  0.105024\n",
      "5     F  0.161516  0.144489\n"
     ]
    }
   ],
   "source": [
    "def han_jaccard(chap_list):\n",
    "    grams = [set(re.findall(r'[\\u4e00-\\u9fff]', ch)) for ch in chap_list]\n",
    "    jacs = []\n",
    "    for g1, g2 in zip(grams, grams[1:]):\n",
    "        union = g1 | g2\n",
    "        jacs.append(len(g1 & g2) / len(union) if union else 0.0)\n",
    "    return jacs, np.mean(jacs)\n",
    "\n",
    "def adjacent_cosine(chap_list, new_order=None):\n",
    "    vec = CountVectorizer(\n",
    "        tokenizer=lambda s: re.findall(r'[\\u4e00-\\u9fff]', s),\n",
    "        ngram_range=(1,2),\n",
    "        min_df=1,\n",
    "        binary=True)\n",
    "    tfidf = vec.fit_transform(chap_list)\n",
    "    sim   = cosine_similarity(tfidf)\n",
    "    if new_order is None:\n",
    "        new_order = range(80)          # 原版顺序\n",
    "    else:\n",
    "        new_order = np.asarray(new_order)\n",
    "    return np.mean([sim[i, j] for i, j in zip(new_order, new_order[1:])])\n",
    "\n",
    "# 评估主循环\n",
    "def evaluate_one(lbl, txt_raw, order):\n",
    "    \"\"\"返回平滑度 + 相邻余弦（与重排阶段完全一致）\"\"\"\n",
    "    # 1. 按重排索引取字符串（顺序已对齐）\n",
    "    reord_txt = [txt_raw[i] for i in order]\n",
    "    # 2. 平滑度\n",
    "    _, smooth = han_jaccard(reord_txt)\n",
    "    # 3. 相邻余弦（用已对齐字符串）\n",
    "    vec   = CountVectorizer(tokenizer=lambda s: re.findall(r'[\\u4e00-\\u9fff]', s),\n",
    "                            ngram_range=(1,2), min_df=1, binary=True)\n",
    "    tfidf = vec.fit_transform(reord_txt)\n",
    "    sim   = cosine_similarity(tfidf)\n",
    "    adj_cos = np.mean([sim[i, j] for i, j in zip(range(80), range(1,81))])\n",
    "    return smooth, adj_cos\n",
    "\n",
    "# 主循环（不再读文件，直接喂原始字符串 + 重排索引）\n",
    "results = []\n",
    "for lbl in ['A','B','C','D','E','F']:\n",
    "    txt_raw = inputs_raw[lbl]          # 你前面已定义的原始字符串列表\n",
    "    order   = orders[lbl]              # 你前面已算好的重排索引\n",
    "    smooth, adj_cos = evaluate_one(lbl, txt_raw, order)\n",
    "    results.append({'Group': lbl, 'Smooth': smooth, 'AdjCos': adj_cos})\n",
    "\n",
    "eval_df = pd.DataFrame(results)\n",
    "eval_df.to_csv(out_dir / 'evaluation_6groups_aligned.csv', index=False)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "end_md",
   "metadata": {},
   "source": [
    "---\n",
    "Open in Colab: [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USER/ddj-reorder-jca/blob/main/bilingual_notebook.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
